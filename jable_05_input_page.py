import time  # To calculate runtime
from playwright.sync_api import sync_playwright


def scrape_jable():
    # Capture user input for the start and end page numbers
    start_page = int(input("è«‹è¼¸å…¥èµ·å§‹é ç¢¼ï¼ˆæ­£æ•´æ•¸ï¼‰: "))
    end_page = int(input("è«‹è¼¸å…¥çµæŸé ç¢¼ï¼ˆæ­£æ•´æ•¸ï¼‰: "))

    start_time = time.time()  # Record the starting time
    failed_urls = []  # List to store URLs that failed on the first attempt
    still_failed_urls = []  # List to store URLs that fail after retrying

    with sync_playwright() as p:
        # First attempt to process URLs
        for page_num in range(start_page, end_page + 1):  # Include end_page
            url = f"https://jable.tv/categories/chinese-subtitle/{page_num}/"
            print(f"\nğŸ“Œ æ­£åœ¨è¨ªå•: {url}")

            # Launch browser
            browser = p.chromium.launch(headless=False)  # Change to True for headless mode
            page = browser.new_page()

            try:
                # Load the page
                page.goto(url, wait_until="domcontentloaded", timeout=5000)

                # Wait for the page to fully load
                page.wait_for_load_state("networkidle")

                # Scroll to the bottom to ensure content loads
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(3000)  # Wait for 3 seconds to let content load

                # Extract all video titles and links
                page.wait_for_selector("h6.title a", timeout=1000)
                videos = page.query_selector_all("h6.title a")
                if videos:
                    for video in videos:
                        title = video.inner_text().strip()
                        link = video.get_attribute("href")
                        print(f"{title}, {link}")

                        # Append results to file
                        with open("jable_c.txt", "a", encoding="utf-8") as file:
                            file.write(f"{title}, {link}\n")
                else:
                    print("âš ï¸ æ²’æœ‰æ‰¾åˆ°å½±ç‰‡åˆ—è¡¨ï¼Œå¯èƒ½æ˜¯ç¶²é è¼‰å…¥å¤±æ•—ã€‚")
                    failed_urls.append(url)  # Log the failed URL

            except Exception as e:
                print(f"âŒ éŒ¯èª¤: {url} ç„¡æ³•æ‰¾åˆ° 'h6.title a'ï¼ŒéŒ¯èª¤ä¿¡æ¯: {e}")
                failed_urls.append(url)  # Log the failed URL

            finally:
                # Close the browser
                browser.close()

        # Retry the failed URLs
        print("\nğŸ”„ é‡æ–°å˜—è©¦è¨ªå•å¤±æ•—çš„ç¶²å€...")
        for failed_url in failed_urls:
            print(f"\nğŸ“Œ æ­£åœ¨é‡æ–°è¨ªå•: {failed_url}")

            # Relaunch the browser
            browser = p.chromium.launch(headless=False)
            page = browser.new_page()

            try:
                # Load the failed page
                page.goto(failed_url, wait_until="domcontentloaded", timeout=5000)
                page.wait_for_load_state("networkidle")

                # Scroll and wait
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(3000)

                # Extract videos
                page.wait_for_selector("h6.title a", timeout=1000)
                videos = page.query_selector_all("h6.title a")
                if videos:
                    for video in videos:
                        title = video.inner_text().strip()
                        link = video.get_attribute("href")
                        print(f"{title}, {link}")
                        with open("jable_c.txt", "a", encoding="utf-8") as file:
                            file.write(f"{title}, {link}\n")
                else:
                    print("âš ï¸ ä»ç„¶æ²’æœ‰æ‰¾åˆ°å½±ç‰‡åˆ—è¡¨ã€‚")
                    still_failed_urls.append(failed_url)  # Log the URL as still failed

            except Exception as e:
                print(f"âŒ é‡æ–°è¨ªå•å¤±æ•—: {failed_url}ï¼ŒéŒ¯èª¤ä¿¡æ¯: {e}")
                still_failed_urls.append(failed_url)  # Log the URL as still failed

            finally:
                # Close the browser
                browser.close()

    # Calculate total runtime and average time
    end_time = time.time()
    total_time = end_time - start_time
    total_pages = len(range(start_page, end_page + 1)) + len(failed_urls)  # Correctly calculate total pages
    average_time = total_time / total_pages

    print(f"\nğŸ“Š çˆ¬å–å®Œæˆï¼ç¸½ç”¨æ™‚: {total_time:.2f} ç§’ï¼Œå¹³å‡æ¯é ç”¨æ™‚: {average_time:.2f} ç§’")

    # Log the still failed URLs
    if still_failed_urls:
        print("\nâŒ ä»¥ä¸‹ç¶²å€ä»ç„¶ç„¡æ³•è¨ªå•:")
        for failed in still_failed_urls:
            print(failed)
    else:
        print("\nâœ… æ‰€æœ‰ç¶²å€å‡å·²æˆåŠŸè¨ªå•ï¼")


# Execute the script
scrape_jable()
